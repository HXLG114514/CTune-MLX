# Ctune-mlx

## introduce

<p align="center">
</Also-coder-5
</p>

This project is used to train models on mlx. It solved the problem of not being able to use unsloth on Apple Silicon, and the generated ``model_new` automatically implements format conversion!

## environment

### 1.Conda installation

Recommended installation **Miniconda** lighter

[Miniconda Download](https://docs.conda.io/en/latest/miniconda.html)

Verify after installation:

```bash
CONDA -Version
```

### 2. Create an environment
```bash
Conda Create -N CTUNE-MLX Python 3.10.18
```

### 3. Enter the project folder and enter the conda environment
```bash
Activate Ctune-MLX
```
## 4. Installation dependencies
```bash
pip install -r requirements.txt
```

## 5. Download the model
```bash
python download.py
```
It takes some time to download the model ()

## 6. Convert format
You can modify the content of ``train_data-new.json` is the training fine-tuning dataset, you can modify it to achieve changes in the fine-tuning effect
Reference ``Training Sample Prompt.txt``

### Convert
```bash
python convert_to_mlx_jsonl.py
```
After running, wait for ``train_data-new.jsonl`` to appear

## 7. Training the model

### You can modify its learning rate and training parameters in ``train_mlx.sh``

```bash
mlx_lm.lora \
--model ./model/qwen2 \
--train \
â€”Then date \
--adapter-path ./mlx_adapters \
--batch-size 1 \
--ifers 3600 \
--learning-rate 8e-5 \
--seed 3407 \
--steps-per-report 20 \
--steps-per-eval 360 \
--save-every 360 \
--num-layers 16 \
--max-seq-length 2048 \
--optimizer adam \
--grad-checkpoint
```
The default effect can actually be run without adjustment
```bash
./train_mlx.sh
```
Then you can run ``inference.py``
## 8. Run the model
```bash
python infference.py
```
You can experience your model at the terminal

## Convert gguf

Download ``llama-cpp``

```bash
git clone https://github.com/ggerganov/llama.cpp.git
```

### First convert the model to gguf and the accuracy is f32
```bash
python llama.cpp/convert_hf_to_gguf.py ./model_new --outfile model_new.gguf --outtype f32
```
### Quantize the converted model into q5_1 (the model is smaller and the loss is high)
```bash
.\llama-tools\llama-quantize.exe coder5_mini_f16.gguf coder5_mini_q5_1.gguf q5_1
```

## License
Please refer to the license documents in the project, please pay attention to the license in commercial use and other fields!